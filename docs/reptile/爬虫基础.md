- [爬虫基础](#jc)
- [网页获取](#wyhq)
- [网页解析](#wyjx)
- [数据存储](#sjcc)

# <a id="jc">爬虫基础</a>

#### 什么是爬虫？

爬虫本质上是一种自动收集互联网上信息的手段，代替人工的方式。

可以通过http包发送请求获取响应进行解析或者采用selenium等框架模拟浏览器获取响应。

爬虫的流程一般包括三部分：

- 获取网页
  - requests、urlib、selenium
  - 多线程/进程、登录验证、IP代理
- 提取数据
  - 正则表达式
  - BeautifulSoup
  - lxml
  - XPath
- 数据存储
  - txt、csv
  - mysql、mongodb、Redis

#### 第一个简单爬虫

```python
# 第一个爬虫
import requests
from bs4 import BeautifulSoup

# 获取网页
url = "http://www.baidu.com"
headers = {'Host': 'www.baidu.com', 'Connection': 'keep-alive', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36', 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9'}
res = requests.get(url, headers=headers)
print(res.text)

# 提取信息
bs = BeautifulSoup(res.text, "html.parser")
title = bs.find("title").text.strip()
print(title)

# 存储数据
with open('test.txt', 'a+') as f:
    f.write(title)
```

此程序请求百度首页，并获取了标题写入test.txt文件。

# <a id="wyhq">网页获取</a>

#### 静态页面获取

静态网页通过http相关库发送请求即可获取响应，python标准http为urlib，使用不如requests方便，一般采用requests开发。

- requests安装

> pip install requests

- requests基本使用
  - res.text：服务器响应内容
  - res.url：生成的url
  - res.encoding：服务器内容使用的编码
  - res.status_code：状态码
  - res.content：响应体
  - res.json()：requests内置的Json解析器

```python
import requests

# 获取网页
url = "http://www.baidu.com"
headers = {'Host': 'www.baidu.com', 'Connection': 'keep-alive', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36', 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9'}
res = requests.get(url, headers=headers)
# 获得响应内容
print(res.text)
# 获取响应状态码
print(res.status_code)
# 获取编码方式
print(res.encoding)

---------------------------------------------

.....
<script src="http://ss.bdimg.com/static/superman/js/components/hotsearch-2ae76631d8.js"></script>
    </body>
        
	</html>
200
utf-8
```

- requests携带参数

```python
# 获取网页
url = "http://www.baidu.com"
headers = {'Host': 'www.baidu.com', 'Connection': 'keep-alive', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36', 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9'}
params = {
    "key1": "value1",
    "key2": "value2"
}
res = requests.get(url, headers=headers,params=params)
# 打印url
print(res.url)

---------------------------
https://www.baidu.com/?key1=value1&key2=value2
```

- requests发送post，data为表单数据

```python
import requests

# 获取网页
url = "http://www.baidu.com"
headers = {'Host': 'www.baidu.com', 'Connection': 'keep-alive', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36', 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9'}
params = {
    "key1": "value1",
    "key2": "value2"
}
res = requests.post(url, headers=headers, params=params, data=params)
```

- requests设置超时

```python
# 获取网页
url = "http://www.baidu.com"
headers = {'Host': 'www.baidu.com', 'Connection': 'keep-alive', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36', 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9'}
params = {
    "key1": "value1",
    "key2": "value2"
}
res = requests.get(url, timeout=0.01, headers=headers, params=params)
-----------------------------
requests.exceptions.ConnectTimeout: HTTPConnectionPool(host='www.baidu.com', port=80): Max retries exceeded wit
```

- requests获取响应内容

  - res.content：获取响应内容，二进制
  - res.text：获取响应内容，自动解压、编码
  - res.json（）：requests内置的json解析器

- cookie和session

  - res.cookies：获取cookies参数
  - requests.session()：获取一个session对象，和requests对象一样，但会自动携带cookies信息，保持登录状态

- requests处理HTTPS请求

  - response = requests.get("https://www.baidu.com/", verify=True)：忽略SSL验证

- requests挂代理

  - 如果需要使用代理，可以通过为任意请求方法提供 `proxies` 参数来配置单个请求

  > ```python
  > # 根据协议类型，选择不同的代理
  > proxies = {
  >   "http": "http://12.34.56.79:9527",
  >   "https": "http://12.34.56.79:9527",
  > }
  > 
  > response = requests.get("http://www.baidu.com", proxies = proxies)
  > ```

- 实战：爬取豆瓣top250电影名称

```python
# 爬取豆瓣top250
def get_movies():
    headers = {
        "Host": "movie.douban.com",
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36"
    }
    titles = []
    for i in range(0, 10):
        url = "https://movie.douban.com/top250?start=" + str(i*25)
        res = requests.get(url, headers=headers, timeout=10)
        print("第{}个请求的响应为{}!".format(i+1, res.status_code))
        # print(res.text)
        sp = BeautifulSoup(res.text, "html.parser")
        head_list = sp.find_all("div", class_="hd")
        for item in head_list:
            title = item.a.span.text.strip()
            titles.append(title)
    return titles

movies = get_movies()
print(movies)
-----------------------------------
['肖申克的救赎', '霸王别姬', '阿甘正传', '这个杀手不太冷', '美丽人生', '泰坦尼克号', '千与千寻', '辛德勒的名单'....]
```

#### 动态页面获取

静态网页的所有内容都在html源代码中，获取到响应即可解析。而动态网页由于采用Js和Ajax，部分内容通过JS动态生成，为了采集这部分内容，可以采用以下两种方式：

- 通过浏览器审查元素的真实地址，即Ajax请求的接口，直接去接口抓取
- 使用Selenium模拟浏览器，执行Ajax请求，解析JS代码，抓取响应

----------

**示例**

爬取http://www.santostang.com/2018/07/04/hello-world/网站的评论请求，通过检查发现，该评论由Ajax加载，Url为https://api-zero.livere.com/v1/comments/list

```python
import requests
import json

url = "https://api-zero.livere.com/v1/comments/list?callback=jQuery11240996539589482099_1595813733080&limit=10&repSeq=4272904&requestPath=%2Fv1%2Fcomments%2Flist&consumerSeq=1020&livereSeq=28583&smartloginSeq=5154&code=&_=1595813733082"
headers = {
    "Host": "api-zero.livere.com",
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36"
}
res = requests.get(url, headers=headers)
print(res.text)
json_str = res.text[res.text.find('{'):-2]
print(json_str)
json_data = json.loads(json_str)
print(type(json_data))
print(json.dumps(json_data, sort_keys=True, indent=4, separators=(',', ':'), ensure_ascii=False))
# 获取所有评论
comments = json_data["results"]["children"]
for comment in comments:
    print("用户{}评论了：{}".format(comment["name"], comment["content"]))
-------------------------------------------
用户岭峰评论了：今天安装了 lxml 后，很快就执行成功了。谢谢。
用户初幻科技丶艾克评论了：你的python环境缺少lxml解析器……我也是刚发现的，百度了一下安装上就好了，windows shell配置好的话一句 pip3 install lxml 就ok了
用户一评论了：有解决方法了，黏贴评论的地址时最后的"code&="要删掉，也就是要跟书上的格式一模一样才行
用户一评论了：我也是
```

- Selenium介绍

> [Selenium](http://www.seleniumhq.org/)是一个Web的自动化测试工具，最初是为网站自动化测试而开发的，类型像我们玩游戏用的按键精灵，可以按指定的命令自动操作，不同是Selenium 可以直接运行在浏览器上，它支持所有主流的浏览器（包括PhantomJS这些无界面的浏览器）。
>
> Selenium 可以根据我们的指令，让浏览器自动加载页面，获取需要的数据，甚至页面截屏，或者判断网站上某些动作是否发生。
>
> Selenium 自己不带浏览器，不支持浏览器的功能，它需要与第三方浏览器结合在一起才能使用。但是我们有时候需要让它内嵌在代码中运行，所以我们可以用一个叫 PhantomJS 的工具代替真实的浏览器。
>
> [PhantomJS](http://phantomjs.org) 是一个基于Webkit的“无界面”(headless)浏览器，它会把网站加载到内存并执行页面上的 JavaScript，因为不会展示图形界面，所以运行起来比完整的浏览器要高效。
>
> 如果我们把 Selenium 和 PhantomJS 结合在一起，就可以运行一个非常强大的网络爬虫了，这个爬虫可以处理 JavaScrip、Cookie、headers，以及任何我们真实用户需要做的事情。
>
> PhantomJS 官方参考文档：[http://phantomjs.org/documentation](http://phantomjs.org/documentation/)

- Selenium入门

>  pip install selenium

```python
# 导入 webdriver
from selenium import webdriver

# 调用键盘按键操作时需要引入的Keys包
from selenium.webdriver.common.keys import Keys

# 调用环境变量指定的PhantomJS浏览器创建浏览器对象
driver = webdriver.PhantomJS()

# 如果没有在环境变量指定PhantomJS位置
# driver = webdriver.PhantomJS(executable_path="./phantomjs"))

# get方法会一直等到页面被完全加载，然后才会继续程序，通常测试会在这里选择 time.sleep(2)
driver.get("http://www.baidu.com/")

# 获取页面名为 wrapper的id标签的文本内容
data = driver.find_element_by_id("wrapper").text

# 打印数据内容
print data

# 打印页面标题 "百度一下，你就知道"
print driver.title

# 生成当前页面快照并保存
driver.save_screenshot("baidu.png")

# id="kw"是百度搜索输入框，输入字符串"长城"
driver.find_element_by_id("kw").send_keys(u"长城")

# id="su"是百度搜索按钮，click() 是模拟点击
driver.find_element_by_id("su").click()

# 获取新的页面快照
driver.save_screenshot("长城.png")

# 打印网页渲染后的源代码
print driver.page_source

# 获取当前页面Cookie
print driver.get_cookies()

# ctrl+a 全选输入框内容
driver.find_element_by_id("kw").send_keys(Keys.CONTROL,'a')

# ctrl+x 剪切输入框内容
driver.find_element_by_id("kw").send_keys(Keys.CONTROL,'x')

# 输入框重新输入内容
driver.find_element_by_id("kw").send_keys("itcast")

# 模拟Enter回车键
driver.find_element_by_id("su").send_keys(Keys.RETURN)

# 清除输入框内容
driver.find_element_by_id("kw").clear()

# 生成新的页面快照
driver.save_screenshot("itcast.png")

# 获取当前url
print driver.current_url

# 关闭当前页面，如果只有一个页面，会关闭浏览器
# driver.close()

# 关闭浏览器
driver.quit()
```

- Selenium页面操作

  - driver.execute_script()：执行JS脚本
  - driver.switch_to.frame()：解析iframe
  - driver.switch_to.default_content() ：转回未解析iframe的原文本
  - find_element_by_css_selector：通过元素的class选择
  - find_element_by_xpath：通过xpath选择
  - find_element_by_id：通过元素的id选择
  - find_element_by_name：通过元素的name选择
  - find_element_by_tag_name：通过元素的名称选择
  - find_element_by_link_text：通过链接地址的文本选择，如选择下一页的URL
  - getAttribute：获取指定属性的值

  > 其中，xpath和css_selector是比较好的方法

  - click()：模拟点击
  - Clear()：清除元素的内容
  - send_keys()：模拟按键输入
  - Submit()：提交表单

  > Selenium参考文档：http://selenium-python.readthedocs.io/index.html

- Selenium模拟鼠标动作操作

  ```python
  #导入 ActionChains 类
  from selenium.webdriver import ActionChains
  
  # 鼠标移动到 ac 位置
  ac = driver.find_element_by_xpath('element')
  ActionChains(driver).move_to_element(ac).perform()
  
  
  # 在 ac 位置单击
  ac = driver.find_element_by_xpath("elementA")
  ActionChains(driver).move_to_element(ac).click(ac).perform()
  
  # 在 ac 位置双击
  ac = driver.find_element_by_xpath("elementB")
  ActionChains(driver).move_to_element(ac).double_click(ac).perform()
  
  # 在 ac 位置右击
  ac = driver.find_element_by_xpath("elementC")
  ActionChains(driver).move_to_element(ac).context_click(ac).perform()
  
  # 在 ac 位置左键单击hold住
  ac = driver.find_element_by_xpath('elementF')
  ActionChains(driver).move_to_element(ac).click_and_hold(ac).perform()
  
  # 将 ac1 拖拽到 ac2 位置
  ac1 = driver.find_element_by_xpath('elementD')
  ac2 = driver.find_element_by_xpath('elementE')
  ActionChains(driver).drag_and_drop(ac1, ac2).perform()
  ```

- Selenium操作下拉框

  ```python
  # 导入 Select 类
  from selenium.webdriver.support.ui import Select
  
  # 找到 name 的选项卡
  select = Select(driver.find_element_by_name('status'))
  
  # 
  select.select_by_index(1)
  select.select_by_value("0")
  select.select_by_visible_text(u"未审核")
  
  # 以上是三种选择下拉框的方式，它可以根据索引来选择，可以根据值来选择，可以根据文字来选择。注意：
  # index 索引从 0 开始
  # value是option标签的一个属性值，并不是显示在下拉框中的值
  # visible_text是在option标签文本的值，是显示在下拉框的值
  
  # 全部取消
  select.deselect_all()
  ```

- Selenium控制弹窗

  - 获取弹窗信息：alert = driver.switch_to_alert()

- Selenium页面切换

  - 切换指定窗口：driver.switch_to.window("this is window name")

  - 获取每个窗口操作对象：

    ```python
    for handle in driver.window_handles:
        driver.switch_to_window(handle)
    ```

- Selenium实现页面前进和后退

  - ```python
    driver.forward()     #前进
    driver.back()        # 后退
    ```

- Selenium获取Cookies

```python
for cookie in driver.get_cookies():
    print "%s -> %s" % (cookie['name'], cookie['value'])
# 删除cookies
# By name
driver.delete_cookie("CookieName")

# all
driver.delete_all_cookies()
```

- Selenium实现页面等待

> 现在的网页越来越多采用了 Ajax 技术，这样程序便不能确定何时某个元素完全加载出来了。如果实际页面等待时间过长导致某个dom元素还没出来，但是你的代码直接使用了这个WebElement，那么就会抛出NullPointer的异常。
>
> 为了避免这种元素定位困难而且会提高产生 ElementNotVisibleException 的概率。所以 Selenium 提供了两种等待方式，一种是隐式等待，一种是显式等待。
>
> 隐式等待是等待特定的时间，显式等待是指定某一条件直到这个条件成立时继续执行。

**显式等待**：显式等待指定某个条件，然后设置最长等待时间。如果在这个时间还没有找到元素，那么便会抛出异常了。

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
# WebDriverWait 库，负责循环等待
from selenium.webdriver.support.ui import WebDriverWait
# expected_conditions 类，负责条件出发
from selenium.webdriver.support import expected_conditions as EC

driver = webdriver.Chrome()
driver.get("http://www.xxxxx.com/loading")
try:
    # 页面一直循环，直到 id="myDynamicElement" 出现
    element = WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.ID, "myDynamicElement"))
    )
finally:
    driver.quit()

```

**隐式等待**：隐式等待比较简单，就是简单地设置一个等待时间，单位为秒。

```python
from selenium import webdriver

driver = webdriver.Chrome()
driver.implicitly_wait(10) # seconds
driver.get("http://www.xxxxx.com/loading")
myDynamicElement = driver.find_element_by_id("myDynamicElement")
```

- Selenium控制页面加载

使用Selenium爬取动态网页时，往往需要整个网页加载出来后才能开始爬取内容，速度往往比较慢。因此在实际使用中，为了提高性能，通常通过以下手段加快Selenium的爬取速度：

- 控制CSS的加载
- 控制图片文件的加载
- 控制JS的运行

```python
# 获取谷歌浏览器驱动
options = Options()
prefs = {
    "profile.default_content_setting_values": {
        # 控制图片加载
        "images": 2,
        # 控制CSS加载
        "permissions.default.stylesheet": 2,
        # 控制JS运行
        "javascript": 2
    }
}
options.add_experimental_option("prefs", prefs)
browser = webdriver.Chrome(options=options, executable_path=r'C:\Users\lizhaoxuan\Desktop\tmp\chromedriver.exe')
browser.get("http://www.santostang.com/2018/07/04/hello-world/")
# 定位到iframe框架里
browser.switch_to.frame(browser.find_element_by_css_selector("iframe[title='livere']"))
comment = browser.find_element_by_css_selector("div.reply-content")
content = comment.find_element_by_tag_name('p')
print(comment.text)
browser.close()
```

- Selenium实践

爬取Airbnb的西安房子数据：https://www.airbnb.cn/s/Xian--China/homes

```python
# Selenium获取Airbnb的西安房屋数据
from selenium import webdriver
import json

# 创建Chrome浏览器
chrome = webdriver.Chrome(executable_path=r'C:\Users\lizhaoxuan\Desktop\tmp\chromedriver.exe')
# 打开Airbnb页面
chrome.get("https://www.airbnb.cn/s/Xian--China/homes?refinement_paths%5B%5D=%2Fhomes&screen_size=large")
# 获取所有房屋
try:
    house_list = chrome.find_element_by_css_selector("div._14csrlku")
except Exception as e:
    try:
        house_list = chrome.find_elements_by_css_selector("div._8ssblpx")
    except Exception as e:
        raise Exception("数据获取失败...")
# 获取每个房屋的详细数据
res = []
for house in house_list:
    # 名称
    name = house.find_element_by_css_selector("div._qrfr9x5").text
    # 价格
    price = house.find_element_by_css_selector("div._1ixtnfc").find_elements_by_tag_name("span")[1].text
    # 标签
    tags = house.find_elements_by_css_selector("span._faldii7")
    tag_list = []
    for tag in tags:
        tag_list.append(tag.find_element_by_tag_name("span").text)
    # 图片url
    img = chrome.find_element_by_xpath("//*[@id='FMP-target']/div[2]/div/div/div/a/div/img")
    img_src = img.get_attribute("src")
    house_dict = {
        "name": name,
        "price": price,
        "tags": tag_list,
        "img": img_src
    }
    res.append(house_dict)
# Json序列化 标准格式
jsonStr = json.dumps(res, sort_keys=True, indent=4, separators=(',', ': '), ensure_ascii=False)
print(jsonStr)
chrome.close()
---------------------------------------
[
    {
        "img": "https://z1.muscache.cn/im/pictures/aaadb496-0c50-4242-abbe-42af4f2b6ae3.jpg?aki_policy=large",
        "name": "「等风来」16min至地铁2号线|钟鼓楼|回民街|世纪金花|开元|交通便利",
        "price": "￥168",
        "tags": [
            "整套公寓 · 1室1卫1床",
            "新房源",
            "超赞房东",
            "灵活退改",
            "自助入住",
            "暖心房东"
        ]
    },
    {
        "img": "https://z1.muscache.cn/im/pictures/aaadb496-0c50-4242-abbe-42af4f2b6ae3.jpg?aki_policy=large",
        "name": "【醉清风】凤五地铁站/西安北站/未央湖/赛高商圈/大明宫遗址公园/市图书馆/原木日式大两居",
        "price": "￥468",
        "tags": [
            "整套公寓 · 2室1卫2床",
            "新房源",
            "超赞房东",
            "灵活退改",
            "自助入住",
            "可以做饭",
            "暖心房东"
        ]
    }
]
```

# <a id="wyjx">网页解析</a>

> 一般来讲对我们而言，需要抓取的是某个网站或者某个应用的内容，提取有用的价值。内容一般分为两部分，非结构化的数据 和 结构化的数据。
>
> - 非结构化的数据处理
>   - 文本、电话号码、邮箱地址
>     - 正则表达式
>   - HTML文本
>     - 正则表达式
>     - XPath
>     - CSS选择器
> - 结构化数据处理
>   - Json文件
>   - XML文件
>     - 转化成Python类型（xmltodict）
>     - XPath
>     - CSS选择器
>     - 正则表达式

#### 正则表达式解析网页

> 正则表达式，又称规则表达式，通常被用来检索、替换那些符合某个模式(规则)的文本。
>
> 正则表达式是对字符串操作的一种逻辑公式，就是用事先定义好的一些特定字符、及这些特定字符的组合，组成一个"规则字符串"，这个"规则字符串"用来表达对字符串的一种过滤逻辑。

- 正则表达式常用规则：

![](http://rocks526.top/lzx/5.2.png)

- Python中的正则表达式

> 在 Python 中，我们可以使用内置的 re 模块来使用正则表达式。
>
> 有一点需要特别注意的是，正则表达式使用 对特殊字符进行转义，所以如果我们要使用原始字符串，只需加一个 r 前缀

**re模块常用操作**

- compile 函数用于编译正则表达式，生成一个 Pattern 对象，它的一般使用形式如下：

```python
import re

# 将正则表达式编译成 Pattern 对象
pattern = re.compile(r'\d+')
```

1. 使用 `compile()` 函数将正则表达式的字符串形式编译为一个 `Pattern` 对象
2. 通过 `Pattern` 对象提供的一系列方法对文本进行匹配查找，获得匹配结果，一个 Match 对象。
3. 最后使用 `Match` 对象提供的属性和方法获得信息，根据需要进行其他的操作

在上面，我们已将一个正则表达式编译成 Pattern 对象，接下来，我们就可以利用 pattern 的一系列方法对文本进行匹配查找了。

Pattern 对象的一些常用方法主要有：

> - match 方法：从起始位置开始查找，一次匹配
> - search 方法：从任何位置开始查找，一次匹配
> - findall 方法：全部匹配，返回列表
> - finditer 方法：全部匹配，返回迭代器
> - split 方法：分割字符串，返回列表
> - sub 方法：替换

------

- match 方法用于查找字符串的头部（也可以指定起始位置），它是一次匹配，只要找到了一个匹配的结果就返回，而不是查找所有匹配的结果。它的一般使用形式如下：

```python
match(string[, pos[, endpos]])
```

其中，string 是待匹配的字符串，pos 和 endpos 是可选参数，指定字符串的起始和终点位置，默认值分别是 0 和 len (字符串长度)。因此，当你不指定 pos 和 endpos 时，match 方法默认匹配字符串的头部。

当匹配成功时，返回一个 Match 对象，如果没有匹配上，则返回 None。

```python
>>> import re
>>> pattern = re.compile(r'\d+')  # 用于匹配至少一个数字

>>> m = pattern.match('one12twothree34four')  # 查找头部，没有匹配
>>> print (m)
None

>>> m = pattern.match('one12twothree34four', 2, 10) # 从'e'的位置开始匹配，没有匹配
>>> print (m)
None

>>> m = pattern.match('one12twothree34four', 3, 10) # 从'1'的位置开始匹配，正好匹配
>>> print (m)                                         # 返回一个 Match 对象
<_sre.SRE_Match object at 0x10a42aac0>

>>> m.group(0)   # 可省略 0
'12'
>>> m.start(0)   # 可省略 0
3
>>> m.end(0)     # 可省略 0
5
>>> m.span(0)    # 可省略 0
(3, 5)
```

在上面，当匹配成功时返回一个 Match 对象，其中：

- group([group1, ...]) 方法用于获得一个或多个分组匹配的字符串，当要获得整个匹配的子串时，可直接使用 group() 或 group(0)；
- start([group]) 方法用于获取分组匹配的子串在整个字符串中的起始位置（子串第一个字符的索引），参数默认值为 0；
- end([group]) 方法用于获取分组匹配的子串在整个字符串中的结束位置（子串最后一个字符的索引+1），参数默认值为 0；
- span([group]) 方法返回 (start(group), end(group))。

------

- search 方法用于查找字符串的任何位置，它也是一次匹配，只要找到了一个匹配的结果就返回，而不是查找所有匹配的结果，它的一般使用形式如下：

```python
search(string[, pos[, endpos]])
```

其中，string 是待匹配的字符串，pos 和 endpos 是可选参数，指定字符串的起始和终点位置，默认值分别是 0 和 len (字符串长度)。

当匹配成功时，返回一个 Match 对象，如果没有匹配上，则返回 None。

```python
>>> import re
>>> pattern = re.compile('\d+')
>>> m = pattern.search('one12twothree34four')  # 这里如果使用 match 方法则不匹配
>>> m
<_sre.SRE_Match object at 0x10cc03ac0>
>>> m.group()
'12'
>>> m = pattern.search('one12twothree34four', 10, 30)  # 指定字符串区间
>>> m
<_sre.SRE_Match object at 0x10cc03b28>
>>> m.group()
'34'
>>> m.span()
(13, 15)
```

------

- findall方法：

上面的 match 和 search 方法都是一次匹配，只要找到了一个匹配的结果就返回。然而，在大多数时候，我们需要搜索整个字符串，获得所有匹配的结果。

findall 方法的使用形式如下：

>  findall(string[, pos[, endpos]])

其中，string 是待匹配的字符串，pos 和 endpos 是可选参数，指定字符串的起始和终点位置，默认值分别是 0 和 len (字符串长度)。

findall 以列表形式返回全部能匹配的子串，如果没有匹配，则返回一个空列表。

```python
import re
pattern = re.compile(r'\d+')   # 查找数字

result1 = pattern.findall('hello 123456 789')
result2 = pattern.findall('one1two2three3four4', 0, 10)

print (result1)
print (result2)
---------------------------------------------
['123456', '789']
['1', '2']
```

------

- finditer方法

finditer 方法的行为跟 findall 的行为类似，也是搜索整个字符串，获得所有匹配的结果。但它返回一个顺序访问每一个匹配结果（Match 对象）的迭代器。

```python
# -*- coding: utf-8 -*-

import re
pattern = re.compile(r'\d+')

result_iter1 = pattern.finditer('hello 123456 789')
result_iter2 = pattern.finditer('one1two2three3four4', 0, 10)

print (type(result_iter1))
print (type(result_iter2))

print 'result1...'
for m1 in result_iter1:   # m1 是 Match 对象
    print ('matching string: {}, position: {}'.format(m1.group(), m1.span()))

print 'result2...'
for m2 in result_iter2:
    print ('matching string: {}, position: {}'.format(m2.group(), m2.span()))
-----------------------------------------------------------------------------------
<type 'callable-iterator'>
<type 'callable-iterator'>
result1...
matching string: 123456, position: (6, 12)
matching string: 789, position: (13, 16)
result2...
matching string: 1, position: (3, 4)
matching string: 2, position: (7, 8)
```

------

- split 方法按照能够匹配的子串将字符串分割后返回列表，它的使用形式如下：

>  split(string[, maxsplit])

其中，maxsplit 用于指定最大分割次数，不指定将全部分割。

```python
import re
p = re.compile(r'[\s\,\;]+')
print (p.split('a,b;; c   d'))
-----------------------------------------
['a', 'b', 'c', 'd']
```

--------

- sub 方法用于替换。它的使用形式如下：

>  sub(repl, string[, count])

其中，repl 可以是字符串也可以是一个函数：

- 如果 repl 是字符串，则会使用 repl 去替换字符串每一个匹配的子串，并返回替换后的字符串，另外，repl 还可以使用 id 的形式来引用分组，但不能使用编号 0；
- 如果 repl 是函数，这个方法应当只接受一个参数（Match 对象），并返回一个字符串用于替换（返回的字符串中不能再引用分组）。
- count 用于指定最多替换次数，不指定时全部替换。

```python
import re
p = re.compile(r'(\w+) (\w+)') # \w = [A-Za-z0-9]
s = 'hello 123, hello 456'

print (p.sub(r'hello world', s))  # 使用 'hello world' 替换 'hello 123' 和 'hello 456'
print (p.sub(r'\2 \1', s))        # 引用分组

def func(m):
    print(m)
    return 'hi' + ' ' + m.group(2) #group(0) 表示本身，group(1)表示hello，group(2) 表示后面的数字

print (p.sub(func, s))  #多次sub，每次sub的结果传递给func
print (p.sub(func, s, 1))         # 最多替换一次
------------------------------------------------------------
hello world, hello world
123 hello, 456 hello
hi 123, hi 456
hi 123, hello 456
```

--------------

- 匹配中文

在某些情况下，我们想匹配文本中的汉字，有一点需要注意的是，中文的 unicode 编码范围 主要在 [u4e00-u9fa5]，这里说主要是因为这个范围并不完整，比如没有包括全角（中文）标点，不过，在大部分情况下，应该是够用的。

假设现在想把字符串 title = u'你好，hello，世界' 中的中文提取出来，可以这么做：

> 注意到，我们在正则表达式前面加上了两个前缀 ur，其中 r 表示使用原始字符串，u 表示是 unicode 字符串。

```python
import re

title = '你好，hello，世界'
pattern = re.compile(r'[\u4e00-\u9fa5]+')
result = pattern.findall(title)

print (result)
--------------------------------
['你好', '世界']
```

- 贪婪模式和非贪婪模式
  - 贪婪模式：在整个表达式匹配成功的前提下，尽可能多的匹配 ( * )；
  - 非贪婪模式：在整个表达式匹配成功的前提下，尽可能少的匹配 ( ? )；
  - **Python里数量词默认是贪婪的。**

示例一 ： 源字符串：`abbbc`

> - 使用贪婪的数量词的正则表达式 `ab*` ，匹配结果： abbb。
>
>   > `*` 决定了尽可能多匹配 b，所以a后面所有的 b 都出现了。
>
> - 使用非贪婪的数量词的正则表达式`ab*?`，匹配结果： a。
>
>   > 即使前面有 `*`，但是 `?` 决定了尽可能少匹配 b，所以没有 b。

- 正则表达式实战

```python

```

