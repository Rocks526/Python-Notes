- [爬虫基础](#jc)
- [网页获取](#wyhq)
- [网页解析](#wyjx)
- [数据存储](#sjcc)



# <a id="jc">爬虫基础</a>

#### 什么是爬虫？

爬虫本质上是一种自动收集互联网上信息的手段，代替人工的方式。

可以通过http包发送请求获取响应进行解析或者采用selenium等框架模拟浏览器获取响应。

爬虫的流程一般包括三部分：

- 获取网页
  - requests、urlib、selenium
  - 多线程/进程、登录验证、IP代理
- 提取数据
  - 正则表达式
  - BeautifulSoup
  - lxml
  - XPath
- 数据存储
  - txt、csv
  - mysql、mongodb、Redis

#### 第一个简单爬虫

```python
# 第一个爬虫
import requests
from bs4 import BeautifulSoup

# 获取网页
url = "http://www.baidu.com"
headers = {'Host': 'www.baidu.com', 'Connection': 'keep-alive', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36', 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9'}
res = requests.get(url, headers=headers)
print(res.text)

# 提取信息
bs = BeautifulSoup(res.text, "html.parser")
title = bs.find("title").text.strip()
print(title)

# 存储数据
with open('test.txt', 'a+') as f:
    f.write(title)
```

此程序请求百度首页，并获取了标题写入test.txt文件。

# <a id="wyhq">网页获取</a>

#### 静态页面获取

静态网页通过http相关库发送请求即可获取响应，python标准http为urlib，使用不如requests方便，一般采用requests开发。

- requests安装

> pip install requests

- requests基本使用
  - res.text：服务器响应内容
  - res.url：生成的url
  - res.encoding：服务器内容使用的编码
  - res.status_code：状态码
  - res.content：响应体
  - res.json()：requests内置的Json解析器

```python
import requests

# 获取网页
url = "http://www.baidu.com"
headers = {'Host': 'www.baidu.com', 'Connection': 'keep-alive', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36', 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9'}
res = requests.get(url, headers=headers)
# 获得响应内容
print(res.text)
# 获取响应状态码
print(res.status_code)
# 获取编码方式
print(res.encoding)

---------------------------------------------

.....
<script src="http://ss.bdimg.com/static/superman/js/components/hotsearch-2ae76631d8.js"></script>
    </body>
        
	</html>
200
utf-8
```

- requests携带参数

```python
# 获取网页
url = "http://www.baidu.com"
headers = {'Host': 'www.baidu.com', 'Connection': 'keep-alive', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36', 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9'}
params = {
    "key1": "value1",
    "key2": "value2"
}
res = requests.get(url, headers=headers,params=params)
# 打印url
print(res.url)

---------------------------
https://www.baidu.com/?key1=value1&key2=value2
```

- requests发送post，data为表单数据

```python
import requests

# 获取网页
url = "http://www.baidu.com"
headers = {'Host': 'www.baidu.com', 'Connection': 'keep-alive', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36', 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9'}
params = {
    "key1": "value1",
    "key2": "value2"
}
res = requests.post(url, headers=headers, params=params, data=params)
```

- requests设置超时

```python
# 获取网页
url = "http://www.baidu.com"
headers = {'Host': 'www.baidu.com', 'Connection': 'keep-alive', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36', 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9'}
params = {
    "key1": "value1",
    "key2": "value2"
}
res = requests.get(url, timeout=0.01, headers=headers, params=params)
-----------------------------
requests.exceptions.ConnectTimeout: HTTPConnectionPool(host='www.baidu.com', port=80): Max retries exceeded wit
```

- 实战：爬取豆瓣top250电影名称

```python
# 爬取豆瓣top250
def get_movies():
    headers = {
        "Host": "movie.douban.com",
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36"
    }
    titles = []
    for i in range(0, 10):
        url = "https://movie.douban.com/top250?start=" + str(i*25)
        res = requests.get(url, headers=headers, timeout=10)
        print("第{}个请求的响应为{}!".format(i+1, res.status_code))
        # print(res.text)
        sp = BeautifulSoup(res.text, "html.parser")
        head_list = sp.find_all("div", class_="hd")
        for item in head_list:
            title = item.a.span.text.strip()
            titles.append(title)
    return titles

movies = get_movies()
print(movies)
-----------------------------------
['肖申克的救赎', '霸王别姬', '阿甘正传', '这个杀手不太冷', '美丽人生', '泰坦尼克号', '千与千寻', '辛德勒的名单'....]
```

#### 动态页面获取

静态网页的所有内容都在html源代码中，获取到响应即可解析。而动态网页由于采用Js和Ajax，内容不再html源代码中，因此可以采用以下两种方式抓取动态网页：

- 通过浏览器审查元素的真实地址，即Ajax请求的接口，直接去接口抓取
- 使用Selenium模拟浏览器，执行Ajax请求，抓取响应

----------

**示例**

爬取http://www.santostang.com/2018/07/04/hello-world/网站的评论请求，通过检查发现，该评论由Ajax加载，Url为https://api-zero.livere.com/v1/comments/list

```python
import requests
import json

url = "https://api-zero.livere.com/v1/comments/list?callback=jQuery11240996539589482099_1595813733080&limit=10&repSeq=4272904&requestPath=%2Fv1%2Fcomments%2Flist&consumerSeq=1020&livereSeq=28583&smartloginSeq=5154&code=&_=1595813733082"
headers = {
    "Host": "api-zero.livere.com",
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36"
}
res = requests.get(url, headers=headers)
print(res.text)
json_str = res.text[res.text.find('{'):-2]
print(json_str)
json_data = json.loads(json_str)
print(type(json_data))
print(json.dumps(json_data, sort_keys=True, indent=4, separators=(',', ':'), ensure_ascii=False))
# 获取所有评论
comments = json_data["results"]["children"]
for comment in comments:
    print("用户{}评论了：{}".format(comment["name"], comment["content"]))
-------------------------------------------
用户岭峰评论了：今天安装了 lxml 后，很快就执行成功了。谢谢。
用户初幻科技丶艾克评论了：你的python环境缺少lxml解析器……我也是刚发现的，百度了一下安装上就好了，windows shell配置好的话一句 pip3 install lxml 就ok了
用户一评论了：有解决方法了，黏贴评论的地址时最后的"code&="要删掉，也就是要跟书上的格式一模一样才行
用户一评论了：我也是
```

- Selenium爬取

> pip install selenium

```python
# 获取谷歌浏览器驱动
browser = webdriver.Chrome(executable_path=r'C:\Users\lizhaoxuan\Desktop\tmp\chromedriver.exe')
browser.get("http://www.santostang.com/2018/07/04/hello-world/")
# 定位到iframe框架里
browser.switch_to.frame(browser.find_element_by_css_selector("iframe[title='livere']"))
comment = browser.find_element_by_css_selector("div.reply-content")
content = comment.find_element_by_tag_name('p')
print(comment.text)
browser.close()
```

- driver.execute_script()：执行JS脚本
- driver.switch_to.frame()：解析iframe
- driver.switch_to.default_content() ：转回未解析iframe的原文本
- find_element_by_css_selector：通过元素的class选择
- find_element_by_xpath：通过xpath选择
- find_element_by_id：通过元素的id选择
- find_element_by_name：通过元素的name选择
- find_element_by_tag_name：通过元素的名称选择
- find_element_by_link_text：通过链接地址选择
- getAttribute：获取指定属性的值

> 其中，xpath和css_selector是比较好的方法

- click()：模拟点击
- Clear()：清除元素的内容
- send_keys()：模拟按键输入
- Submit()：提交表单

> Selenium参考文档：http://selenium-python.readthedocs.io/index.html

- Selenium高级操作

使用Selenium爬取动态网页时，往往需要整个网页加载出来后才能开始爬取内容，速度往往比较慢。因此在实际使用中，为了提高性能，通常通过以下手段加快Selenium的爬取速度：

- 控制CSS的加载
- 控制图片文件的加载
- 控制JS的运行

```python
# 获取谷歌浏览器驱动
options = Options()
prefs = {
    "profile.default_content_setting_values": {
        # 控制图片加载
        "images": 2,
        # 控制CSS加载
        "permissions.default.stylesheet": 2,
        # 控制JS运行
        "javascript": 2
    }
}
options.add_experimental_option("prefs", prefs)
browser = webdriver.Chrome(options=options, executable_path=r'C:\Users\lizhaoxuan\Desktop\tmp\chromedriver.exe')
browser.get("http://www.santostang.com/2018/07/04/hello-world/")
# 定位到iframe框架里
browser.switch_to.frame(browser.find_element_by_css_selector("iframe[title='livere']"))
comment = browser.find_element_by_css_selector("div.reply-content")
content = comment.find_element_by_tag_name('p')
print(comment.text)
browser.close()
```

- Selenium实践

爬取Airbnb的西安房子数据：https://www.airbnb.cn/s/Xian--China/homes

```python
# Selenium获取Airbnb的西安房屋数据
from selenium import webdriver
import json

# 创建Chrome浏览器
chrome = webdriver.Chrome(executable_path=r'C:\Users\lizhaoxuan\Desktop\tmp\chromedriver.exe')
# 打开Airbnb页面
chrome.get("https://www.airbnb.cn/s/Xian--China/homes?refinement_paths%5B%5D=%2Fhomes&screen_size=large")
# 获取所有房屋
try:
    house_list = chrome.find_element_by_css_selector("div._14csrlku")
except Exception as e:
    try:
        house_list = chrome.find_elements_by_css_selector("div._8ssblpx")
    except Exception as e:
        raise Exception("数据获取失败...")
# 获取每个房屋的详细数据
res = []
for house in house_list:
    # 名称
    name = house.find_element_by_css_selector("div._qrfr9x5").text
    # 价格
    price = house.find_element_by_css_selector("div._1ixtnfc").find_elements_by_tag_name("span")[1].text
    # 标签
    tags = house.find_elements_by_css_selector("span._faldii7")
    tag_list = []
    for tag in tags:
        tag_list.append(tag.find_element_by_tag_name("span").text)
    # 图片url
    img = chrome.find_element_by_xpath("//*[@id='FMP-target']/div[2]/div/div/div/a/div/img")
    img_src = img.get_attribute("src")
    house_dict = {
        "name": name,
        "price": price,
        "tags": tag_list,
        "img": img_src
    }
    res.append(house_dict)
# Json序列化 标准格式
jsonStr = json.dumps(res, sort_keys=True, indent=4, separators=(',', ': '), ensure_ascii=False)
print(jsonStr)
chrome.close()
---------------------------------------
[
    {
        "img": "https://z1.muscache.cn/im/pictures/aaadb496-0c50-4242-abbe-42af4f2b6ae3.jpg?aki_policy=large",
        "name": "「等风来」16min至地铁2号线|钟鼓楼|回民街|世纪金花|开元|交通便利",
        "price": "￥168",
        "tags": [
            "整套公寓 · 1室1卫1床",
            "新房源",
            "超赞房东",
            "灵活退改",
            "自助入住",
            "暖心房东"
        ]
    },
    {
        "img": "https://z1.muscache.cn/im/pictures/aaadb496-0c50-4242-abbe-42af4f2b6ae3.jpg?aki_policy=large",
        "name": "【醉清风】凤五地铁站/西安北站/未央湖/赛高商圈/大明宫遗址公园/市图书馆/原木日式大两居",
        "price": "￥468",
        "tags": [
            "整套公寓 · 2室1卫2床",
            "新房源",
            "超赞房东",
            "灵活退改",
            "自助入住",
            "可以做饭",
            "暖心房东"
        ]
    }
]
```

# <a id="wyjx">网页解析</a>

#### 正则表达式解析网页

- 正则表达式常用规则：

![image-20200801161142286](C:\Users\lizhaoxuan\AppData\Roaming\Typora\typora-user-images\image-20200801161142286.png)

- Python中的正则表达式
  - 